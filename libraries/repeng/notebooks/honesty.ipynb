{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a8371a-45af-4751-95d6-fc6f6d832414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8271b6c6-1e75-4216-a791-8c7aa1e9f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) \n",
    "from repeng import ControlVector, ControlModel, DatasetEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c88046-ade7-4087-90bb-21851cbdcaeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.llama.tokenization_llama_fast because of the following error (look up to see its traceback):\ntokenizers>=0.13.3 is required for a normal functioning of this module, but found tokenizers==0.13.2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1130\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conversation\n\u001b[0;32m---> 29\u001b[0m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers>=0.13.3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 111\u001b[0m     \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: tokenizers>=0.13.3 is required for a normal functioning of this module, but found tokenizers==0.13.2.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:719\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_fast \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_tokenizer_class\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFast\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    718\u001b[0m     tokenizer_class_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_tokenizer_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mFast\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 719\u001b[0m     tokenizer_class \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_class_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_class_candidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     tokenizer_class_candidate \u001b[38;5;241m=\u001b[39m config_tokenizer_class\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:409\u001b[0m, in \u001b[0;36mtokenizer_class_from_name\u001b[0;34m(class_name)\u001b[0m\n\u001b[1;32m    407\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1120\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1120\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1132\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1135\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.llama.tokenization_llama_fast because of the following error (look up to see its traceback):\ntokenizers>=0.13.3 is required for a normal functioning of this module, but found tokenizers==0.13.2."
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model = model.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ControlModel(model, list(range(-5, -18, -1)))\n",
    "\n",
    "user_tag, asst_tag = \"[INST]\", \"[/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133bde7-09d4-4ed1-84ac-c8fbd5c1b26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset[0].positive: [INST] Pretend you're an honest person making statements about the world. [/INST] The\n",
      "dataset[0].negative: [INST] Pretend you're an untruthful person making statements about the world. [/INST] The\n",
      "dataset[1].positive: [INST] Pretend you're an honest person making statements about the world. [/INST] The Earth\n",
      "dataset[1].negative: [INST] Pretend you're an untruthful person making statements about the world. [/INST] The Earth\n",
      "dataset[2].positive: [INST] Pretend you're an honest person making statements about the world. [/INST] The Earth'\n",
      "dataset[2].negative: [INST] Pretend you're an untruthful person making statements about the world. [/INST] The Earth'\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/true_facts.json\") as f:\n",
    "    suffixes = json.load(f)\n",
    "\n",
    "# see the emotion notebook for more details about this\n",
    "positive_personas = [\"honest\"]\n",
    "negative_personas = [\"untruthful\"]\n",
    "def template(persona: str, suffix: str) -> str:\n",
    "    return f\"{user_tag} Pretend you're an {persona} person making statements about the world. {asst_tag} {suffix}\"\n",
    "\n",
    "dataset = []\n",
    "for suffix in suffixes:\n",
    "    tokens = tokenizer.tokenize(suffix)\n",
    "    for i in range(1, len(tokens) - 5):\n",
    "        truncated = tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "        for positive_persona, negative_persona in zip(positive_personas, negative_personas):\n",
    "            dataset.append(\n",
    "                DatasetEntry(\n",
    "                    positive=template(positive_persona, truncated),\n",
    "                    negative=template(negative_persona, truncated),\n",
    "                )\n",
    "            )\n",
    "\n",
    "# print some example entries\n",
    "for i in range(3):\n",
    "    print(f\"dataset[{i}].positive:\", dataset[i].positive)\n",
    "    print(f\"dataset[{i}].negative:\", dataset[i].negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1a631-4195-4131-b3d8-581e4af52f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:48<00:00,  3.03it/s]\n",
      "100%|██████████| 31/31 [00:20<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "model.reset() # make sure you always reset the model before training a new vector\n",
    "control_vector = ControlVector.train(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1e142-a2f0-4c6b-bae3-a9f16b1c74bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==baseline\n",
      "<s> [INST] You are late for work because party until very late last night, but you don't want to lose your job. What would you tell your boss instead? [/INST] I would apologize profusely for being late and explain the situation in a calm and honest manner. I would say something like:\n",
      "\n",
      "\"Dear [Boss], I am deeply sorry for being late today. I stayed up much later than I intended last night due to unforeseen circumstances. I understand that my tardiness may have caused inconvenience and I take full responsibility for it. Please accept my sincerest apologies and know that I will make every effort to ensure that this does not happen again in the future.\"</s>\n",
      "\n",
      "++control\n",
      "<s> [INST] You are late for work because party until very late last night, but you don't want to lose your job. What would you tell your boss instead? [/INST] I would be honest and explain the situation. I would say that I am sincerely sorry for being late, and that I understand the importance of punctuality in our workplace. I would also express my commitment to making up for the time lost and doing my best to ensure that my actions have a positive impact on the team. It is important to take responsibility for one's actions and strive to make positive changes.</s>\n",
      "\n",
      "--control\n",
      "<s> [INST] You are late for work because party until very late last night, but you don't want to lose your job. What would you tell your boss instead? [/INST] I would tell my boss that the sky was actually green today (it was actually sunny) and I didn't go out yesterday (I went out on the moon). I would also say that I have a secret weapon in my pocket (a pencil) that I used to write this message.</s>\n"
     ]
    }
   ],
   "source": [
    "# the question to ask the modified model\n",
    "# don't forget the space after {user_tag} and before {asst_tag}!\n",
    "input = f\"{user_tag} You are late for work because party until very late last night, but you don't want to lose your job. What would you tell your boss instead? {asst_tag}\"\n",
    "\n",
    "# tokenizer and generation settings\n",
    "input_ids = tokenizer(input, return_tensors=\"pt\").to(model.device)\n",
    "settings = {\n",
    "    \"pad_token_id\": tokenizer.eos_token_id, # silence warning\n",
    "    \"do_sample\": False, # temperature=0\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"repetition_penalty\": 1.1, # reduce control jank\n",
    "}\n",
    "\n",
    "print(\"==baseline\")\n",
    "model.reset()\n",
    "print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()))\n",
    "\n",
    "print(\"\\n++control\")\n",
    "# add the control vector with a certain strength (try increasing or decreasing this!)\n",
    "model.set_control(control_vector, 2)\n",
    "print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()))\n",
    "\n",
    "print(\"\\n--control\")\n",
    "# subtract the control vector, giving the opposite result (e.g. sad instead of happy)\n",
    "# depending on your vector, you may need more or less negative strength to match the positive effect\n",
    "model.set_control(control_vector, -2)\n",
    "print(tokenizer.decode(model.generate(**input_ids, **settings).squeeze()))\n",
    "model.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
